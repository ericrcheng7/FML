{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Airbnb Listings Data Analysis and Embeddings\n",
    "\n",
    "This notebook:\n",
    "1. Downloads NYC Airbnb listings data from Inside Airbnb\n",
    "2. Cleans the data (handles price column, removes duplicates, selects useful features)\n",
    "3. Generates embeddings for the features using sentence transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import urllib.request\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download NYC Airbnb Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nyc_data(output_dir=\"data\"):\n",
    "    \"\"\"\n",
    "    Download NYC Airbnb listings data from Inside Airbnb\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # URL for NYC listings data\n",
    "    url = \"http://data.insideairbnb.com/united-states/ny/new-york-city/2024-12-04/data/listings.csv.gz\"\n",
    "    output_path = os.path.join(output_dir, \"listings.csv.gz\")\n",
    "    \n",
    "    print(f\"Downloading NYC listings data from {url}...\")\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "    print(f\"Downloaded data to {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Download the data\n",
    "data_path = download_nyc_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(data_path, compression='gzip')\n",
    "print(f\"Loaded {len(df)} listings with {len(df.columns)} columns\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Cleaning Data ===\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "# 1. Clean the price column\n",
    "if 'price' in df.columns:\n",
    "    print(\"\\nBefore cleaning price column:\")\n",
    "    print(df['price'].head())\n",
    "    \n",
    "    df['price'] = df['price'].str.replace('$', '', regex=False)\n",
    "    df['price'] = df['price'].str.replace(',', '', regex=False)\n",
    "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "    \n",
    "    print(\"\\nAfter cleaning price column:\")\n",
    "    print(df['price'].head())\n",
    "    print(f\"Cleaned price column - removed $ and commas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove duplicates\n",
    "original_count = len(df)\n",
    "df = df.drop_duplicates(subset=['id'])\n",
    "duplicates_removed = original_count - len(df)\n",
    "print(f\"Removed {duplicates_removed} duplicate listings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Select useful room features to keep\n",
    "useful_columns = [\n",
    "    'id',\n",
    "    'name',\n",
    "    'description',\n",
    "    'neighborhood_overview',\n",
    "    'host_id',\n",
    "    'host_name',\n",
    "    'neighbourhood_cleansed',\n",
    "    'neighbourhood_group_cleansed',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'property_type',\n",
    "    'room_type',\n",
    "    'accommodates',\n",
    "    'bathrooms_text',\n",
    "    'bedrooms',\n",
    "    'beds',\n",
    "    'amenities',\n",
    "    'price',\n",
    "    'minimum_nights',\n",
    "    'maximum_nights',\n",
    "    'number_of_reviews',\n",
    "    'review_scores_rating',\n",
    "    'review_scores_accuracy',\n",
    "    'review_scores_cleanliness',\n",
    "    'review_scores_checkin',\n",
    "    'review_scores_communication',\n",
    "    'review_scores_location',\n",
    "    'review_scores_value',\n",
    "    'instant_bookable',\n",
    "    'reviews_per_month'\n",
    "]\n",
    "\n",
    "# Only keep columns that exist in the dataframe\n",
    "existing_columns = [col for col in useful_columns if col in df.columns]\n",
    "df = df[existing_columns]\n",
    "print(f\"Selected {len(existing_columns)} useful columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Remove listings with missing essential data\n",
    "df = df.dropna(subset=['name', 'price'])\n",
    "print(f\"Removed listings with missing name or price\")\n",
    "\n",
    "# 5. Remove invalid price values\n",
    "df = df[df['price'] > 0]\n",
    "print(f\"Removed listings with invalid prices (<= 0)\")\n",
    "\n",
    "# 6. Remove extreme outliers in price\n",
    "df = df[df['price'] <= 10000]\n",
    "print(f\"Removed listings with extreme prices (> $10,000)\")\n",
    "\n",
    "print(f\"\\nFinal shape after cleaning: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Price Statistics:\")\n",
    "print(df['price'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['price'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Price Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df[df['price'] <= 500]['price'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Price Distribution (Price <= $500)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room type distribution\n",
    "if 'room_type' in df.columns:\n",
    "    print(\"\\nRoom Type Distribution:\")\n",
    "    print(df['room_type'].value_counts())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['room_type'].value_counts().plot(kind='bar')\n",
    "    plt.xlabel('Room Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Room Types')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighborhood distribution\n",
    "if 'neighbourhood_group_cleansed' in df.columns:\n",
    "    print(\"\\nNeighborhood Group Distribution:\")\n",
    "    print(df['neighbourhood_group_cleansed'].value_counts())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['neighbourhood_group_cleansed'].value_counts().plot(kind='bar')\n",
    "    plt.xlabel('Neighborhood Group')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Listings by Neighborhood Group')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property type distribution\n",
    "if 'property_type' in df.columns:\n",
    "    print(\"\\nTop 10 Property Types:\")\n",
    "    print(df['property_type'].value_counts().head(10))\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df['property_type'].value_counts().head(10).plot(kind='bar')\n",
    "    plt.xlabel('Property Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Top 10 Property Types')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Embeddings\n",
    "\n",
    "We'll use the `sentence-transformers` library with the `nomic-ai/nomic-embed-text-v1.5` model, as learned in lab3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_for_embedding(row):\n",
    "    \"\"\"\n",
    "    Create a text representation of a listing for embedding generation\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Add listing name\n",
    "    if pd.notna(row.get('name')):\n",
    "        parts.append(f\"Name: {row['name']}\")\n",
    "    \n",
    "    # Add description\n",
    "    if pd.notna(row.get('description')):\n",
    "        parts.append(f\"Description: {row['description']}\")\n",
    "    \n",
    "    # Add neighborhood overview\n",
    "    if pd.notna(row.get('neighborhood_overview')):\n",
    "        parts.append(f\"Neighborhood: {row['neighborhood_overview']}\")\n",
    "    \n",
    "    # Add property type and room type\n",
    "    if pd.notna(row.get('property_type')):\n",
    "        parts.append(f\"Property Type: {row['property_type']}\")\n",
    "    \n",
    "    if pd.notna(row.get('room_type')):\n",
    "        parts.append(f\"Room Type: {row['room_type']}\")\n",
    "    \n",
    "    # Add amenities\n",
    "    if pd.notna(row.get('amenities')):\n",
    "        parts.append(f\"Amenities: {row['amenities']}\")\n",
    "    \n",
    "    # Add location info\n",
    "    if pd.notna(row.get('neighbourhood_cleansed')):\n",
    "        parts.append(f\"Location: {row['neighbourhood_cleansed']}\")\n",
    "    \n",
    "    return \" \".join(parts)\n",
    "\n",
    "# Example of text representation\n",
    "print(\"Example text representation:\")\n",
    "print(create_text_for_embedding(df.iloc[0])[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentence transformer model\n",
    "print(\"Loading model: nomic-ai/nomic-embed-text-v1.5\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text representations for each listing\n",
    "print(\"Creating text representations...\")\n",
    "texts = df.apply(create_text_for_embedding, axis=1).tolist()\n",
    "\n",
    "# Add the \"search_document:\" prefix as per the model's recommendation\n",
    "texts_prefixed = [\"search_document: \" + text for text in texts]\n",
    "\n",
    "print(f\"Created {len(texts_prefixed)} text representations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "# Note: This may take several minutes depending on the number of listings\n",
    "print(f\"Generating embeddings for {len(texts_prefixed)} listings...\")\n",
    "embeddings = model.encode(\n",
    "    texts_prefixed,\n",
    "    batch_size=32,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Generated embeddings with shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings to dataframe\n",
    "df['embedding'] = list(embeddings)\n",
    "\n",
    "print(f\"Added embeddings to dataframe\")\n",
    "print(f\"Each listing now has a {df['embedding'].iloc[0].shape[0]}-dimensional embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Embeddings\n",
    "\n",
    "Let's verify that the embeddings are normalized and test similarity between listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if embeddings are normalized\n",
    "norms = np.linalg.norm(embeddings[:10], axis=1)\n",
    "print(f\"Norms of first 10 embeddings: {norms}\")\n",
    "print(f\"Are embeddings normalized? {np.allclose(norms, 1.0, atol=1e-3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find similar listings using embeddings\n",
    "def find_similar_listings(query_idx, top_k=5):\n",
    "    \"\"\"\n",
    "    Find the most similar listings to a given listing\n",
    "    \"\"\"\n",
    "    query_embedding = df['embedding'].iloc[query_idx]\n",
    "    \n",
    "    # Convert embeddings to matrix\n",
    "    embedding_matrix = np.stack(df['embedding'].values)\n",
    "    \n",
    "    # Calculate similarity scores (dot product since embeddings are normalized)\n",
    "    scores = embedding_matrix @ query_embedding\n",
    "    \n",
    "    # Get top k indices (excluding the query itself)\n",
    "    top_k_indices = np.argsort(scores)[::-1][1:top_k+1]\n",
    "    \n",
    "    print(f\"\\nQuery Listing (Index {query_idx}):\")\n",
    "    print(f\"Name: {df.iloc[query_idx]['name']}\")\n",
    "    print(f\"Room Type: {df.iloc[query_idx].get('room_type', 'N/A')}\")\n",
    "    print(f\"Price: ${df.iloc[query_idx]['price']}\")\n",
    "    \n",
    "    print(f\"\\nTop {top_k} Most Similar Listings:\")\n",
    "    for i, idx in enumerate(top_k_indices, 1):\n",
    "        print(f\"\\n{i}. (Similarity: {scores[idx]:.4f})\")\n",
    "        print(f\"   Name: {df.iloc[idx]['name']}\")\n",
    "        print(f\"   Room Type: {df.iloc[idx].get('room_type', 'N/A')}\")\n",
    "        print(f\"   Price: ${df.iloc[idx]['price']}\")\n",
    "\n",
    "# Test with a random listing\n",
    "find_similar_listings(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and embedded data\n",
    "output_path = \"data/nyc_listings_cleaned_embedded.parquet\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df.to_parquet(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}\")\n",
    "print(f\"\\nFinal dataset contains {len(df)} listings\")\n",
    "print(f\"Each listing has a {df['embedding'].iloc[0].shape[0]}-dimensional embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "1. ✅ Downloaded NYC Airbnb listings data from Inside Airbnb\n",
    "2. ✅ Cleaned the dataset:\n",
    "   - Fixed the price column (removed $ and commas, converted to numeric)\n",
    "   - Removed duplicate listings\n",
    "   - Selected useful room features\n",
    "   - Removed listings with invalid or missing data\n",
    "   - Filtered out extreme price outliers\n",
    "3. ✅ Generated embeddings using the `nomic-ai/nomic-embed-text-v1.5` model from sentence-transformers\n",
    "4. ✅ Saved the cleaned and embedded data for future use\n",
    "\n",
    "The embeddings can now be used for:\n",
    "- Semantic search of listings\n",
    "- Finding similar properties\n",
    "- Clustering listings by features\n",
    "- Building recommendation systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
